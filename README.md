# ğŸ§  NLP â€” Du Texte Brut Ã  BERT

> Quatre notebooks Jupyter qui explorent le **Natural Language Processing** de A Ã  Z :
> du dÃ©coupage de phrases simples jusqu'au fine-tuning d'un Transformer prÃ©-entraÃ®nÃ©.

---

## ï¿½ï¸ Vue d'Ensemble

```
TP1  PrÃ©traitement       â†’  Comment un ordinateur "lit" du texte
TP2  BOW & TF-IDF        â†’  Comment transformer du texte en chiffres
TP3  Word2Vec & FastText  â†’  Comment donner du "sens" aux mots
TP4  BERT                 â†’  Comment utiliser un modÃ¨le de deep learning prÃ©-entraÃ®nÃ©
```

Chaque notebook est **autonome** : il contient le code, les explications, et les rÃ©sultats d'exÃ©cution.

---

## ğŸ“‚ Structure

```
â”œâ”€â”€ Solutions/
â”‚   â”œâ”€â”€ TP1_Pretraitement.ipynb       (38 cellules)
â”‚   â”œâ”€â”€ TP2_BOW_TFIDF.ipynb           (36 cellules)
â”‚   â”œâ”€â”€ TP3_Word2Vec_FastText.ipynb    (59 cellules)
â”‚   â””â”€â”€ TP4_BERT.ipynb                (18 cellules)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ alice_wonderland.txt          Texte complet d'Alice au pays des merveilles
â”‚   â”œâ”€â”€ spam.csv                      5 572 emails (spam / ham)
â”‚   â”œâ”€â”€ train_tweets.csv              Tweets pour sentiment analysis
â”‚   â”œâ”€â”€ test_tweets.csv               Tweets (test)
â”‚   â””â”€â”€ Comment Spam.xls              Commentaires spam YouTube
â”‚
â”œâ”€â”€ pyproject.toml                    DÃ©pendances Python
â””â”€â”€ README.md
```

---

## ğŸš€ Installation

```bash
# Avec uv (recommandÃ©)
uv run jupyter lab

# Ou classiquement
pip install pandas numpy nltk scikit-learn gensim matplotlib seaborn transformers datasets torch accelerate
jupyter lab
```

Puis ouvrir les notebooks dans `Solutions/` dans l'ordre TP1 â†’ TP4.

---

## ğŸ“˜ TP1 â€” PrÃ©traitement de Texte

**Objectif** : Apprendre Ã  prÃ©parer du texte brut avant toute analyse NLP.

### Ce qu'on y fait

- **Tokenisation** : DÃ©couper un texte en phrases, puis en mots.
  On compare 3 tokenizers (TreebankWord, WordPunct, Regex) et on observe comment chacun gÃ¨re
  l'apostrophe, la ponctuation et les contractions (`can't`, `Alice's`).

- **Stemming vs Lemmatisation** : Deux mÃ©thodes pour ramener un mot Ã  sa racine.
  Le stemmer coupe mÃ©caniquement (`running` â†’ `run`, mais aussi `universities` â†’ `univers`).
  Le lemmatiseur utilise un dictionnaire et comprend la grammaire (`better` â†’ `good` si on lui dit que c'est un adjectif).

- **Stopwords** : Les mots vides (`the`, `is`, `a`) dominent les frÃ©quences.
  On montre avec des graphiques que les supprimer rÃ©vÃ¨le les vrais mots-clÃ©s d'un texte.

- **N-grams** : Au-delÃ  du mot unique â€” les bigrammes (`New York`), trigrammes (`not very good`)
  capturent le contexte. On observe qu'au-delÃ  de 4-grams, les sÃ©quences sont souvent uniques.

### Concepts clÃ©s

> Un token n'est pas forcÃ©ment un mot. C'est l'unitÃ© minimale que le modÃ¨le voit.
> Le choix du tokenizer change complÃ¨tement ce que le modÃ¨le comprend.

---

## ğŸ“˜ TP2 â€” Bag of Words & TF-IDF

**Objectif** : Transformer du texte en vecteurs numÃ©riques pour pouvoir faire de la classification.

### Ce qu'on y fait

- **Bag of Words (BOW)** : Chaque document = un vecteur de frÃ©quences de mots.
  Simple mais efficace. On explore l'impact de `max_features` (limiter le vocabulaire aux N mots
  les plus frÃ©quents) sur la prÃ©cision d'un classifieur.

- **Classification spam/ham** : On entraÃ®ne Naive Bayes et SVM sur 5 500 emails.
  SVM est gÃ©nÃ©ralement meilleur car il gÃ¨re mieux les espaces de haute dimension.

- **TF-IDF** : PondÃ¨re les mots par leur raretÃ© dans le corpus.
  Le mot `free` dans un spam a un score TF-IDF Ã©levÃ© car il est frÃ©quent dans ce document
  mais rare dans le corpus global. On construit un pipeline complet :
  `CountVectorizer â†’ TfidfTransformer â†’ LogisticRegression`.

- **Calcul manuel de TF-IDF** : On recalcule tout Ã©tape par Ã©tape (Document Frequency, IDF,
  normalisation L2) pour comprendre ce que sklearn fait en coulisses.
  On vÃ©rifie que notre rÃ©sultat est identique Ã  `TfidfVectorizer`.

- **SimilaritÃ© documentaire** : Heatmap de similaritÃ© cosinus entre 8 documents thÃ©matiques.
  Les documents du mÃªme thÃ¨me (weather, animals, food) se ressemblent davantage.

- **Clustering hiÃ©rarchique** : Dendrogramme qui regroupe automatiquement les documents
  (et les mots) par proximitÃ© sÃ©mantique, sans supervision.

### Concepts clÃ©s

> TF-IDF = "ce mot est-il important **pour ce document** par rapport Ã  l'ensemble ?"
> Un mot prÃ©sent partout (comme `the`) a un IDF proche de 0.
> Un mot rare et spÃ©cifique a un IDF Ã©levÃ©.

---

## ğŸ“˜ TP3 â€” Word Embeddings & Sentiment Analysis

**Objectif** : Passer des comptages de mots Ã  des **reprÃ©sentations sÃ©mantiques** :
un mot = un vecteur dense qui capture son sens.

### Ce qu'on y fait

#### Word2Vec
- **CBOW** (Continuous Bag of Words) : PrÃ©dire un mot Ã  partir de son contexte.
- **Skip-gram** : PrÃ©dire le contexte Ã  partir d'un mot.
- On entraÃ®ne les deux sur *Alice au Pays des Merveilles* et on compare les rÃ©sultats
  avec diffÃ©rentes dimensions de vecteurs (`vector_size = 2, 10, 500`).
- On charge aussi le modÃ¨le prÃ©-entraÃ®nÃ© **Google News** (3 millions de mots, 300 dimensions)
  pour tester des analogies (`king - man + woman â‰ˆ queen`).

#### FastText
- Fonctionne comme Word2Vec mais au niveau **sous-mot** (n-grams de caractÃ¨res).
  Avantage : il peut traiter des mots jamais vus (`unfriendliest` â†’ `un` + `friend` + `li` + ...).

#### Doc2Vec
- ReprÃ©senter un **document entier** par un seul vecteur (moyenne des vecteurs de ses mots).
- On utilise ces vecteurs pour clusteriser des documents par thÃ¨me et on mesure la qualitÃ©
  avec l'Adjusted Rand Index.

#### Visualisation
- **PCA** : Projection linÃ©aire rapide, prÃ©serve les grandes distances.
- **t-SNE** : Projection non-linÃ©aire, rÃ©vÃ¨le les clusters locaux.
  Le paramÃ¨tre `perplexity` contrÃ´le l'Ã©quilibre local/global.

#### Sentiment Analysis (pipeline complet)
On compare systÃ©matiquement **16 combinaisons** :

|  | BoW | TF-IDF | Word2Vec | Doc2Vec |
|---|:---:|:---:|:---:|:---:|
| **Logistic Regression** | âœ… | âœ… | âœ… | âœ… |
| **SVM** | âœ… | âœ… | âœ… | âœ… |
| **Random Forest** | âœ… | âœ… | âœ… | âœ… |
| **XGBoost/GB** | âœ… | âœ… | âœ… | âœ… |

RÃ©sultats visualisÃ©s via des heatmaps d'Accuracy et F1-Score.

### Concepts clÃ©s

> Word2Vec apprend que `roi` et `reine` sont proches car ils apparaissent
> dans des contextes similaires. Il ne sait pas ce que ces mots *signifient*,
> mais il capture leurs relations d'usage.

---

## ğŸ“˜ TP4 â€” BERT (Transformers)

**Objectif** : Utiliser un modÃ¨le de deep learning prÃ©-entraÃ®nÃ© pour classifier des sentiments,
en exploitant le **transfer learning**.

### Ce qu'on y fait

- **Tokenisation WordPiece** : BERT dÃ©coupe les mots rares en sous-unitÃ©s
  (`unbelievable` â†’ `un`, `##bel`, `##iev`, `##able`). Il ajoute aussi des tokens spÃ©ciaux
  `[CLS]` (dÃ©but) et `[SEP]` (fin).

- **Transfer Learning** : BERT a Ã©tÃ© prÃ©-entraÃ®nÃ© sur Wikipedia + BookCorpus (3.3 milliards de mots).
  Il a dÃ©jÃ  "lu" tellement de texte qu'il comprend la grammaire, le contexte et les nuances.
  On ne fait que le **fine-tuner** (ajuster la derniÃ¨re couche) sur notre tÃ¢che spÃ©cifique.

- **Fine-tuning** : On prend `bert-base-uncased` (110M paramÃ¨tres) et on l'adapte
  Ã  la classification de sentiments sur le dataset IMDB (critiques de films).
  MÃªme avec 500 exemples et 1 seule Ã©poque, il obtient des rÃ©sultats raisonnables.

- **InfÃ©rence** : On teste le modÃ¨le sur 4 phrases personnalisÃ©es et on observe
  les prÃ©dictions avec leur score de confiance.

### Concepts clÃ©s

> BERT est **bidirectionnel** : pour comprendre le mot `bank`, il regarde Ã  la fois
> les mots avant ET aprÃ¨s. C'est ce qui le diffÃ©rencie des modÃ¨les prÃ©cÃ©dents
> qui lisaient le texte de gauche Ã  droite uniquement.
>
> Le transfer learning est la raison pour laquelle BERT fonctionne avec peu de donnÃ©es :
> il part avec 110M de paramÃ¨tres dÃ©jÃ  entraÃ®nÃ©s, pas de zÃ©ro.

---

## ğŸ“Š Progression des Concepts

```
Texte brut
    â”‚
    â–¼
[TP1] Tokenisation, Nettoyage
    â”‚   "Le chat mange" â†’ ["chat", "mange"]
    â–¼
[TP2] Vectorisation (BOW / TF-IDF)
    â”‚   ["chat", "mange"] â†’ [0, 1, 0, 1, 0, ...]  (vecteur creux)
    â–¼
[TP3] Embeddings (Word2Vec / FastText)
    â”‚   "chat" â†’ [0.23, -0.41, 0.87, ...]  (vecteur dense, sÃ©mantique)
    â–¼
[TP4] Transformers (BERT)
        "chat" â†’ [contextualisÃ© selon la phrase entiÃ¨re]
```

---

## ğŸ› ï¸ Stack Technique

| Lib | Usage |
|---|---|
| `nltk` | Tokenisation, stemming, lemmatisation, stopwords |
| `scikit-learn` | Vectorisation (BOW, TF-IDF), classification, PCA, t-SNE |
| `gensim` | Word2Vec, FastText, Doc2Vec |
| `transformers` | BERT (Hugging Face) |
| `torch` | Backend deep learning |
| `matplotlib` / `seaborn` | Visualisations |
| `pandas` / `numpy` | Manipulation de donnÃ©es |
