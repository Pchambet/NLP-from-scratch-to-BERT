# ğŸ§  NLP â€” Travaux Pratiques

> SÃ©rie de 4 TPs couvrant les fondamentaux du **Natural Language Processing**, du prÃ©traitement de texte brut jusqu'au fine-tuning de BERT.

---

## ğŸ“‚ Structure du Projet

```
TP-Final/
â”œâ”€â”€ Solutions/                          â† Les 4 notebooks (Ã  exÃ©cuter)
â”‚   â”œâ”€â”€ TP1_Pretraitement.ipynb
â”‚   â”œâ”€â”€ TP2_BOW_TFIDF.ipynb
â”‚   â”œâ”€â”€ TP3_Word2Vec_FastText.ipynb
â”‚   â””â”€â”€ TP4_BERT.ipynb
â”‚
â”œâ”€â”€ data/                               â† Datasets utilisÃ©s
â”‚   â”œâ”€â”€ alice_wonderland.txt            (texte pour tokenisation/Word2Vec)
â”‚   â”œâ”€â”€ spam.csv                        (emails spam/ham pour TP2)
â”‚   â”œâ”€â”€ Comment Spam.xls               (variante spam)
â”‚   â”œâ”€â”€ train_tweets.csv               (tweets pour analyse de sentiments)
â”‚   â””â”€â”€ test_tweets.csv
â”‚
â”œâ”€â”€ Chapter 1 - Pretreatment/           â† Sujets PDF (rÃ©fÃ©rence)
â”œâ”€â”€ Chapter 2 - frequency/
â”œâ”€â”€ Chapter 3 - Prediction/
â”œâ”€â”€ Chapter 4 - DNN/
â”‚
â”œâ”€â”€ pyproject.toml                      â† DÃ©pendances (gÃ©rÃ© par uv)
â””â”€â”€ README.md
```

---

## ğŸš€ Installation & ExÃ©cution

### PrÃ©requis

- **Python 3.11+**
- **[uv](https://docs.astral.sh/uv/)** (gestionnaire de packages, recommandÃ©)

### Lancer les notebooks

```bash
# 1. Cloner le projet
cd TP-Final

# 2. Lancer Jupyter (uv installe tout automatiquement)
uv run jupyter lab
```

Ouvrir les notebooks dans `Solutions/` et les exÃ©cuter dans l'ordre (TP1 â†’ TP4).

### Alternative sans uv

```bash
pip install pandas numpy nltk scikit-learn gensim seaborn matplotlib transformers datasets torch accelerate jupyterlab
jupyter lab
```

---

## ğŸ“˜ Contenu des TPs

### TP1 â€” PrÃ©traitement de Texte

| ThÃ¨me | Ce qu'on fait |
|---|---|
| **Tokenisation** | DÃ©coupage en phrases et en mots (NLTK) |
| **Comparaison de Tokenizers** | TreebankWordTokenizer vs WordPunctTokenizer â€” quelles diffÃ©rences ? |
| **RegexTokenizer** | DÃ©finir ses propres rÃ¨gles, gestion de l'apostrophe |
| **Stemming vs Lemmatisation** | PorterStemmer vs WordNetLemmatizer, impact du POS tag, erreurs de frappe |
| **Stopwords** | Filtrage FR/EN, impact sur la frÃ©quence des mots |
| **N-grams** | GÃ©nÃ©ration (1 Ã  6-grams), analyse de raretÃ© |
| **Analyse frÃ©quentielle** | Distribution des mots avec et sans stopwords (graphiques) |

**Corpus** : *Alice in Wonderland*, textes en franÃ§ais, corpus Gutenberg

---

### TP2 â€” Bag of Words & TF-IDF

| ThÃ¨me | Ce qu'on fait |
|---|---|
| **BOW** | Vectorisation avec CountVectorizer, rÃ´le de `max_features` |
| **Classification** | MultinomialNB sur spam/ham, comparaison avec SVM |
| **TF-IDF Pipeline** | CountVectorizer â†’ TfidfTransformer â†’ LogisticRegression |
| **Validation croisÃ©e** | Cross-validation + test sur phrases exemples |
| **TF-IDF Manuel** | Calcul Ã©tape par Ã©tape : DF â†’ IDF â†’ TF-IDF brut â†’ normalisation L2 |
| **SimilaritÃ© documentaire** | Heatmap de similaritÃ© cosinus entre documents |
| **Clustering** | Dendrogramme hiÃ©rarchique (documents et mots) |

**Corpus** : `spam.csv` (5 572 emails) + corpus thÃ©matique (weather/animals/food)

---

### TP3 â€” Word2Vec, FastText & Sentiment Analysis

| ThÃ¨me | Ce qu'on fait |
|---|---|
| **Word2Vec** | EntraÃ®nement CBOW et Skip-gram sur *Alice in Wonderland* |
| **Impact de `vector_size`** | Comparaison avec vector_size = 2, 10, 500 |
| **ModÃ¨le prÃ©-entraÃ®nÃ©** | Google News 300 (3M mots), analogies et similaritÃ©s |
| **FastText** | EntraÃ®nement sur corpus Brown, gestion des mots inconnus (OOV) |
| **Visualisation** | PCA et t-SNE (rÃ´le de la perplexity) |
| **Doc2Vec** | Vecteur moyen d'un document, clustering K-Means |
| **Clustering documents** | Dendrogramme hiÃ©rarchique, Adjusted Rand Index |
| **Sentiment Analysis** | Pipeline complet : nettoyage â†’ 4 embeddings Ã— 4 modÃ¨les (voir ci-dessous) |

#### Pipeline Sentiment Analysis (16 combinaisons)

|  | BoW | TF-IDF | Word2Vec | Doc2Vec |
|---|:---:|:---:|:---:|:---:|
| **Logistic Regression** | âœ… | âœ… | âœ… | âœ… |
| **SVM** | âœ… | âœ… | âœ… | âœ… |
| **Random Forest** | âœ… | âœ… | âœ… | âœ… |
| **XGBoost** | âœ… | âœ… | âœ… | âœ… |

RÃ©sultats comparÃ©s via heatmaps (Accuracy + F1-Score).

---

### TP4 â€” BERT (Transformers)

| ThÃ¨me | Ce qu'on fait |
|---|---|
| **Tokenisation WordPiece** | DÃ©coupage en sous-mots, tokens spÃ©ciaux [CLS]/[SEP] |
| **Fine-tuning** | bert-base-uncased sur IMDB (analyse de sentiment) |
| **Transfer Learning** | Pourquoi BERT marche avec peu de donnÃ©es |
| **Ã‰valuation** | Accuracy sur le test set |
| **InfÃ©rence** | Test sur 4 phrases personnalisÃ©es avec score de confiance |

**Corpus** : IMDB (25K reviews, sous-Ã©chantillonnÃ© Ã  500 pour la dÃ©mo CPU)

---

## âš ï¸ Notes Importantes

- **TP3** : Le tÃ©lÃ©chargement du modÃ¨le Google News (~1.5 GB) peut prendre du temps Ã  la premiÃ¨re exÃ©cution.
- **TP4** : L'entraÃ®nement BERT sur CPU prend ~5 min avec 500 exemples / 1 Ã©poque. Pour de meilleurs rÃ©sultats, augmenter `num_samples` et `num_train_epochs`.
- **Ordre d'exÃ©cution** : Les notebooks sont indÃ©pendants, mais il est recommandÃ© de les suivre dans l'ordre (TP1 â†’ TP4) pour la progression pÃ©dagogique.

---

## ğŸ“š Ressources

Les sujets originaux (PDF) sont dans les dossiers `Chapter 1` Ã  `Chapter 4`. Chaque notebook couvre **tous les PDFs** de son chapitre :

| Notebook | PDFs couverts |
|---|---|
| TP1 | `TP_NLP_1_pretraitrement.pdf` + `TP_NLP_1_pretraitrement 2.pdf` |
| TP2 | `TP_NLP_2_BOW.pdf` + `TP_NLP_2_TFIDF.pdf` + `TP_NLP_2_other_example.pdf` |
| TP3 | `TP_NLP_3_word2vec.pdf` + `TP_NLP_3_Sentiment_analysis.pdf` + `TP_NLP_3_w2v_FastText.pdf` |
| TP4 | `TP_NLP_4_BERT_sentiment_analysis.pdf` |

---

## ğŸ› ï¸ Stack Technique

| Lib | Usage |
|---|---|
| `nltk` | Tokenisation, stemming, lemmatisation, stopwords, N-grams |
| `scikit-learn` | CountVectorizer, TF-IDF, classifieurs (NB, SVM, RF, LR), PCA, t-SNE |
| `gensim` | Word2Vec, FastText, Doc2Vec, modÃ¨les prÃ©-entraÃ®nÃ©s |
| `transformers` | BERT (tokenizer + modÃ¨le), Trainer API |
| `datasets` | Chargement IMDB |
| `torch` | Backend pour BERT |
| `matplotlib` / `seaborn` | Visualisations |
| `pandas` / `numpy` | Manipulation de donnÃ©es |
