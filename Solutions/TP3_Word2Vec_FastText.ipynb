{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# TP 3 - Word2Vec & FastText\n",
                "\n",
                "L'objectif de ce TP est de mettre en œuvre l'algorithme de word2vec (CBOW et Skip-gram) et d'explorer FastText."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercice 1 – Entrainer son propre word2vec"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Importer les dépendances\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize, word_tokenize\n",
                "import warnings\n",
                "import gensim\n",
                "from gensim.models import Word2Vec\n",
                "\n",
                "warnings.filterwarnings(action='ignore')\n",
                "\n",
                "nltk.download('punkt')\n",
                "\n",
                "# 2. Importer les données et déclarer quelques variables\n",
                "# Lecture du fichier Alice in Wonderland\n",
                "try:\n",
                "    sample = open(\"../data/alice_wonderland.txt\", \"r\")\n",
                "    s = sample.read()\n",
                "except FileNotFoundError:\n",
                "    print(\"Fichier non trouvé. Vérifiez le chemin.\")\n",
                "    s = \"\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Prétraitement des données\n",
                "# Remplacer les sauts de ligne par des espaces\n",
                "f = s.replace(\"\\n\", \" \")\n",
                "\n",
                "data = []\n",
                "# Iterer sur chaque phrase\n",
                "for i in sent_tokenize(f):\n",
                "    temp = []\n",
                "    # Tokeniser la phrase en mots\n",
                "    for j in word_tokenize(i):\n",
                "        temp.append(j.lower())\n",
                "    data.append(temp)\n",
                "\n",
                "print(f\"Nombre de phrases: {len(data)}\")\n",
                "print(f\"Premier exemple: {data[0]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Entrainement du CBOW\n",
                "# Create CBOW model\n",
                "model1 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5)\n",
                "\n",
                "# Print results\n",
                "print(\"Cosine similarity between 'alice' \" + \"and 'wonderland' - CBOW : \", model1.wv.similarity('alice', 'wonderland'))\n",
                "print(\"Cosine similarity between 'alice' \" + \"and 'machines' - CBOW : \", model1.wv.similarity('alice', 'machines') if 'machines' in model1.wv else \"'machines' not in vocab\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Entrainement du Skip-gram\n",
                "# Create Skip Gram model (sg=1)\n",
                "model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5, sg = 1)\n",
                "\n",
                "# Print results\n",
                "print(\"Cosine similarity between 'alice' \" + \"and 'wonderland' - Skip Gram : \", model2.wv.similarity('alice', 'wonderland'))\n",
                "print(\"Cosine similarity between 'alice' \" + \"and 'machines' - Skip Gram : \", model2.wv.similarity('alice', 'machines') if 'machines' in model2.wv else \"'machines' not in vocab\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Question :\n",
                "Jouez avec le paramètre vector_size =[2, 10, 500] sur le Skipgram et CBOW, quel est l’effet sur les distances ?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercice 2 – Utiliser un modèle pré-entrainé\n",
                "*Note: Le téléchargement du modèle peut prendre du temps.*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gensim.downloader as api\n",
                "\n",
                "# Télécharger le modèle pré entrainé (Google News 300)\n",
                "# Attention : c'est un gros fichier (~1.5 Go)\n",
                "try:\n",
                "    wv = api.load('word2vec-google-news-300')\n",
                "    print(\"Modèle chargé !\")\n",
                "except Exception as e:\n",
                "    print(\"Erreur lors du chargement ou téléchargement trop long:\", e)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Trouver la similitude entre 2 mots\n",
                "if 'wv' in locals():\n",
                "    pairs = [\n",
                "        ('car', 'minivan'),\n",
                "        ('car', 'bicycle'),\n",
                "        ('car', 'airplane'),\n",
                "        ('car', 'cereal'),\n",
                "        ('car', 'communism'),\n",
                "    ]\n",
                "    for w1, w2 in pairs:\n",
                "        print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))\n",
                "      \n",
                "    # Jouez un peu avec la similitude des mots\n",
                "    print(wv.most_similar(positive=['car', 'minivan'], topn=5))\n",
                "    print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercice 3 - FastText\n",
                "\n",
                "Utilisation de FastText sur le corpus Brown."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from gensim.models.fasttext import FastText\n",
                "nltk.download('brown')\n",
                "from nltk.corpus import brown\n",
                "\n",
                "# Chargement du corpus Brown\n",
                "brown_tokens = [brown.words(fileids=f) for f in brown.fileids()]\n",
                "\n",
                "print(\"Training FastText model...\")\n",
                "# Instantiation et entrainement\n",
                "ft_model = FastText(vector_size=100, window=5, min_count=5, sentences=brown_tokens, epochs=10)\n",
                "print(\"Training complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Similitudes\n",
                "print(\"Similaire 'nation':\", ft_model.wv.most_similar('nation'))\n",
                "# FastText gère bien les mots inconnus (OOV) grâce aux n-grams de caractères\n",
                "print(\"Similaire 'accomodation' (typo):\", ft_model.wv.most_similar('accomodation'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualisation avec PCA\n",
                "from sklearn.decomposition import PCA\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def plot_pca(model, words):\n",
                "    # Récupérer les vecteurs\n",
                "    vectors = [model.wv[w] for w in words if w in model.wv]\n",
                "    if not vectors:\n",
                "        return\n",
                "        \n",
                "    pca = PCA(n_components=2)\n",
                "    result = pca.fit_transform(vectors)\n",
                "    \n",
                "    plt.figure(figsize=(10, 6))\n",
                "    plt.scatter(result[:, 0], result[:, 1])\n",
                "    \n",
                "    for i, word in enumerate(words):\n",
                "        plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
                "    plt.show()\n",
                "\n",
                "words_to_plot = ['king', 'queen', 'man', 'woman', 'car', 'bicycle', 'bus']\n",
                "plot_pca(ft_model, words_to_plot)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}