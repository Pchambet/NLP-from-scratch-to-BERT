{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 2 - Bag of Words (BOW) & TF-IDF\n",
    "\n",
    "L'objectif de ce TP est la construction d'un BOW pour la détection de SPAM sur une base de données de mails, puis l'implémentation de TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 – BOW (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:38:11.703112Z",
     "iopub.status.busy": "2026-02-14T16:38:11.703011Z",
     "iopub.status.idle": "2026-02-14T16:38:14.445471Z",
     "shell.execute_reply": "2026-02-14T16:38:14.445045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pierre/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/pierre/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/pierre/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 1. Importer les dépendances\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 2. Importer les données\n",
    "# On utilise spam.csv qui semble correspondre à la structure attendue (v1, v2)\n",
    "try:\n",
    "    df = pd.read_csv('../data/spam.csv', encoding='latin-1')\n",
    "except FileNotFoundError:\n",
    "    print(\"Fichier non trouvé. Vérifiez le chemin.\")\n",
    "\n",
    "# Renommer les colonnes pour plus de clarté si nécessaire, ou garder v1/v2 comem dans le TP\n",
    "# Le TP utilise v1 (label) et v2 (text)\n",
    "df = df[['v1', 'v2']]\n",
    "df.columns = ['label', 'text'] # Renommons pour clarté\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:38:14.461044Z",
     "iopub.status.busy": "2026-02-14T16:38:14.460904Z",
     "iopub.status.idle": "2026-02-14T16:38:15.570273Z",
     "shell.execute_reply": "2026-02-14T16:38:15.569869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...   \n",
      "1   ham                      Ok lar... Joking wif u oni...   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3   ham  U dun say so early hor... U c already then say...   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  go jurong point crazy available bugis n great ...  \n",
      "1                            ok lar joking wif u oni  \n",
      "2  free entry wkly comp win fa cup final tkts st ...  \n",
      "3                u dun say early hor u c already say  \n",
      "4                nah think go usf life around though  \n"
     ]
    }
   ],
   "source": [
    "# 3. Prétraitement des données\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(df)):\n",
    "    # Supprimer les caractères non alphabétiques\n",
    "    review = re.sub('[^a-zA-Z]', ' ', df['text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    # Lemmatisation et suppression des stopwords\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if not word in stop_words]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "# Ajoutons le corpus nettoyé au dataframe pour vérification\n",
    "df['cleaned_text'] = corpus\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:38:15.571620Z",
     "iopub.status.busy": "2026-02-14T16:38:15.571553Z",
     "iopub.status.idle": "2026-02-14T16:38:15.599659Z",
     "shell.execute_reply": "2026-02-14T16:38:15.599320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (5572, 2500)\n",
      "Shape of y: (5572,)\n"
     ]
    }
   ],
   "source": [
    "# 4. Implémenter BOW avec sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# max_features=2500 signifie qu'on ne garde que les 2500 mots les plus fréquents\n",
    "# Avantage plus grand : capture plus de nuances, mais risque de sur-apprentissage et coût calculatoire\n",
    "# Avantage plus petit : modèle plus simple, rapide, moins de bruit, mais perte d'info\n",
    "cv = CountVectorizer(max_features=2500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "# 5. Transformation de la variable cible (label)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['label'])\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 – Modélisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:38:15.600728Z",
     "iopub.status.busy": "2026-02-14T16:38:15.600670Z",
     "iopub.status.idle": "2026-02-14T16:38:15.700875Z",
     "shell.execute_reply": "2026-02-14T16:38:15.700488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[957   8]\n",
      " [ 10 140]]\n",
      "Accuracy: 0.9838565022421525\n"
     ]
    }
   ],
   "source": [
    "# 1. Définition de la base d’apprentissage et test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Apprentissage d’un modèle de classification (Teoreme de Bayes)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Prédiction et calcul des performances\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3 - TF-IDF\n",
    "\n",
    "Utilisation de TF-IDF pour pondérer l'importance des termes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:38:15.701801Z",
     "iopub.status.busy": "2026-02-14T16:38:15.701740Z",
     "iopub.status.idle": "2026-02-14T16:38:15.757434Z",
     "shell.execute_reply": "2026-02-14T16:38:15.757124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9695067264573991\n",
      "Confusion Matrix:\n",
      " [[976   3]\n",
      " [ 31 105]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# On repart des données brutes pour utiliser le pipeline complet\n",
    "X_raw = df['text']\n",
    "y_raw = df['label']\n",
    "\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X_raw, y_raw, test_size=0.2, random_state=57)\n",
    "\n",
    "# Définir une data pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english')), # Nettoyage et vectorisation\n",
    "    ('tfidf', TfidfTransformer()), # TF-IDF\n",
    "    ('clf', LogisticRegression()), # Classification\n",
    "])\n",
    "\n",
    "# Exécuter le pipeline\n",
    "text_clf.fit(X_train_raw, y_train_raw)\n",
    "\n",
    "# Prédiction\n",
    "predicted = text_clf.predict(X_test_raw)\n",
    "\n",
    "# Imprimer le rapport de performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test_raw, predicted))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_raw, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:38:15.758425Z",
     "iopub.status.busy": "2026-02-14T16:38:15.758367Z",
     "iopub.status.idle": "2026-02-14T16:38:15.952007Z",
     "shell.execute_reply": "2026-02-14T16:38:15.951619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.96681614 0.96591928 0.96229803 0.95780969 0.96768402]\n",
      "Mean score: 0.9641054334962282\n"
     ]
    }
   ],
   "source": [
    "# Validation croisée\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(text_clf, X_raw, y_raw, cv=5)\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean score:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:38:15.953037Z",
     "iopub.status.busy": "2026-02-14T16:38:15.952977Z",
     "iopub.status.idle": "2026-02-14T16:38:15.955160Z",
     "shell.execute_reply": "2026-02-14T16:38:15.954841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Subscribe to my Youtube Channel!! :)' -> ham\n",
      "'Hi veronica, hope you are doing good' -> ham\n",
      "'Earn money for being online with 0 efforts! ...' -> ham\n"
     ]
    }
   ],
   "source": [
    "# Essayer quelques phrases\n",
    "examples = [\n",
    "    'Subscribe to my Youtube Channel!! :)',\n",
    "    'Hi veronica, hope you are doing good',\n",
    "    'Earn money for being online with 0 efforts! ...'\n",
    "]\n",
    "predictions = text_clf.predict(examples)\n",
    "\n",
    "for text, label in zip(examples, predictions):\n",
    "    print(f\"'{text}' -> {label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
