{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# TP 2 - Bag of Words (BOW) & TF-IDF\n",
                "\n",
                "L'objectif de ce TP est la construction d'un BOW pour la détection de SPAM sur une base de données de mails, puis l'implémentation de TF-IDF."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercice 1 – BOW (Bag of Words)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Importer les dépendances\n",
                "import pandas as pd\n",
                "import nltk\n",
                "import numpy as np\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.tokenize import sent_tokenize, word_tokenize\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "import re\n",
                "\n",
                "nltk.download('stopwords')\n",
                "nltk.download('punkt')\n",
                "nltk.download('wordnet')\n",
                "\n",
                "# 2. Importer les données\n",
                "# On utilise spam.csv qui semble correspondre à la structure attendue (v1, v2)\n",
                "try:\n",
                "    df = pd.read_csv('../data/spam.csv', encoding='latin-1')\n",
                "except FileNotFoundError:\n",
                "    print(\"Fichier non trouvé. Vérifiez le chemin.\")\n",
                "\n",
                "# Renommer les colonnes pour plus de clarté si nécessaire, ou garder v1/v2 comem dans le TP\n",
                "# Le TP utilise v1 (label) et v2 (text)\n",
                "df = df[['v1', 'v2']]\n",
                "df.columns = ['label', 'text'] # Renommons pour clarté\n",
                "\n",
                "print(df.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Prétraitement des données\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "stop_words = set(stopwords.words('english'))\n",
                "\n",
                "corpus = []\n",
                "for i in range(len(df)):\n",
                "    # Supprimer les caractères non alphabétiques\n",
                "    review = re.sub('[^a-zA-Z]', ' ', df['text'][i])\n",
                "    review = review.lower()\n",
                "    review = review.split()\n",
                "    \n",
                "    # Lemmatisation et suppression des stopwords\n",
                "    review = [lemmatizer.lemmatize(word) for word in review if not word in stop_words]\n",
                "    review = ' '.join(review)\n",
                "    corpus.append(review)\n",
                "\n",
                "# Ajoutons le corpus nettoyé au dataframe pour vérification\n",
                "df['cleaned_text'] = corpus\n",
                "print(df.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Implémenter BOW avec sklearn\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "\n",
                "# max_features=2500 signifie qu'on ne garde que les 2500 mots les plus fréquents\n",
                "# Avantage plus grand : capture plus de nuances, mais risque de sur-apprentissage et coût calculatoire\n",
                "# Avantage plus petit : modèle plus simple, rapide, moins de bruit, mais perte d'info\n",
                "cv = CountVectorizer(max_features=2500)\n",
                "X = cv.fit_transform(corpus).toarray()\n",
                "\n",
                "# 5. Transformation de la variable cible (label)\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "le = LabelEncoder()\n",
                "y = le.fit_transform(df['label'])\n",
                "\n",
                "print(\"Shape of X:\", X.shape)\n",
                "print(\"Shape of y:\", y.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercice 2 – Modélisation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Définition de la base d’apprentissage et test\n",
                "from sklearn.model_selection import train_test_split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# 2. Apprentissage d’un modèle de classification (Teoreme de Bayes)\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "model = MultinomialNB()\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "# 3. Prédiction et calcul des performances\n",
                "y_pred = model.predict(X_test)\n",
                "\n",
                "from sklearn.metrics import confusion_matrix, accuracy_score\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "acc = accuracy_score(y_test, y_pred)\n",
                "\n",
                "print(\"Confusion Matrix:\\n\", cm)\n",
                "print(\"Accuracy:\", acc)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercice 3 - TF-IDF\n",
                "\n",
                "Utilisation de TF-IDF pour pondérer l'importance des termes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.feature_extraction.text import TfidfTransformer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.pipeline import Pipeline\n",
                "\n",
                "# On repart des données brutes pour utiliser le pipeline complet\n",
                "X_raw = df['text']\n",
                "y_raw = df['label']\n",
                "\n",
                "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X_raw, y_raw, test_size=0.2, random_state=57)\n",
                "\n",
                "# Définir une data pipeline\n",
                "text_clf = Pipeline([\n",
                "    ('vect', CountVectorizer(stop_words='english')), # Nettoyage et vectorisation\n",
                "    ('tfidf', TfidfTransformer()), # TF-IDF\n",
                "    ('clf', LogisticRegression()), # Classification\n",
                "])\n",
                "\n",
                "# Exécuter le pipeline\n",
                "text_clf.fit(X_train_raw, y_train_raw)\n",
                "\n",
                "# Prédiction\n",
                "predicted = text_clf.predict(X_test_raw)\n",
                "\n",
                "# Imprimer le rapport de performance\n",
                "print(\"Accuracy:\", accuracy_score(y_test_raw, predicted))\n",
                "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_raw, predicted))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validation croisée\n",
                "from sklearn.model_selection import cross_val_score\n",
                "\n",
                "scores = cross_val_score(text_clf, X_raw, y_raw, cv=5)\n",
                "print(\"Cross-validation scores:\", scores)\n",
                "print(\"Mean score:\", np.mean(scores))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Essayer quelques phrases\n",
                "examples = [\n",
                "    'Subscribe to my Youtube Channel!! :)',\n",
                "    'Hi veronica, hope you are doing good',\n",
                "    'Earn money for being online with 0 efforts! ...'\n",
                "]\n",
                "predictions = text_clf.predict(examples)\n",
                "\n",
                "for text, label in zip(examples, predictions):\n",
                "    print(f\"'{text}' -> {label}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}